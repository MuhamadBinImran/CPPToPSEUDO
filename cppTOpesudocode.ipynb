{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t', names=['pseudocode', 'cpp_code', 'workerid', 'probid', 'subid', 'line', 'indent'])\n",
        "    df['pseudocode'] = '<sos> ' + df['pseudocode'] + ' <eos>'\n",
        "    df['cpp_code'] = '<sos> ' + df['cpp_code'] + ' <eos>'\n",
        "    return df[['cpp_code', 'pseudocode']].dropna()\n",
        "\n",
        "train_data = load_data('spoc-train-train.tsv')\n",
        "\n",
        "num_words = 20000\n",
        "max_len = 150\n",
        "\n",
        "cpp_tokenizer = Tokenizer(num_words=num_words, filters='', lower=False)\n",
        "cpp_tokenizer.fit_on_texts(train_data['cpp_code'])\n",
        "X_train = cpp_tokenizer.texts_to_sequences(train_data['cpp_code'])\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
        "\n",
        "pseudocode_tokenizer = Tokenizer(num_words=num_words, filters='', lower=True)\n",
        "pseudocode_tokenizer.fit_on_texts(train_data['pseudocode'])\n",
        "y_train = pseudocode_tokenizer.texts_to_sequences(train_data['pseudocode'])\n",
        "y_train = pad_sequences(y_train, maxlen=max_len, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WepYbd6a07O_",
        "outputId": "0a0933a9-aea8-4988-bec7-aa3ea797d51b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-1fe84f407b7e>:11: DtypeWarning: Columns (2,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep='\\t', names=['pseudocode', 'cpp_code', 'workerid', 'probid', 'subid', 'line', 'indent'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('cpp_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(cpp_tokenizer, f)\n",
        "with open('pseudocode_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(pseudocode_tokenizer, f)\n"
      ],
      "metadata": {
        "id": "602rvhf60-QM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % num_heads == 0\n",
        "        self.depth = d_model // num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.split_heads(self.wq(q), batch_size)\n",
        "        k = self.split_heads(self.wk(k), batch_size)\n",
        "        v = self.split_heads(self.wv(v), batch_size)\n",
        "        attn_output = tf.nn.softmax(tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.depth, tf.float32)))\n",
        "        attn_output = tf.matmul(attn_output, v)\n",
        "        attn_output = tf.transpose(attn_output, perm=[0, 2, 1, 3])\n",
        "        return self.dense(tf.reshape(attn_output, (batch_size, -1, self.d_model)))\n",
        "\n",
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x):\n",
        "        attn_output = self.attention(x, x, x)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff}\n",
        "\n",
        "class TransformerDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.attention1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.attention2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x, enc_output):\n",
        "        attn1 = self.attention1(x, x, x)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "        attn2 = self.attention2(out1, enc_output, enc_output)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "        ffn_output = self.ffn(out2)\n",
        "        return self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"d_model\": self.d_model, \"num_heads\": self.num_heads, \"dff\": self.dff}\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, dff, max_len):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, input_length=max_len)\n",
        "        self.encoder = TransformerEncoder(d_model, num_heads, dff)\n",
        "        self.decoder = TransformerDecoder(d_model, num_heads, dff)\n",
        "        self.final_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        enc_output = self.encoder(self.embedding(inputs))\n",
        "        dec_output = self.decoder(self.embedding(inputs), enc_output)\n",
        "        return self.final_layer(dec_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dff\": self.dff,\n",
        "            \"max_len\": self.max_len,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n"
      ],
      "metadata": {
        "id": "8ZGWNwJMz0Rl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(num_words, d_model=128, num_heads=4, dff=512, max_len=max_len)\n",
        "transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "transformer.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "transformer.save('transformer_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C0xHR-p1ZpY",
        "outputId": "0b321ff4-6b5e-4723-8ac6-97cc964d5b00"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 99ms/step - accuracy: 0.9556 - loss: 0.6735 - val_accuracy: 0.9662 - val_loss: 0.1964\n",
            "Epoch 2/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 93ms/step - accuracy: 0.9685 - loss: 0.1766 - val_accuracy: 0.9666 - val_loss: 0.1877\n",
            "Epoch 3/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 93ms/step - accuracy: 0.9697 - loss: 0.1583 - val_accuracy: 0.9666 - val_loss: 0.1866\n",
            "Epoch 4/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 93ms/step - accuracy: 0.9702 - loss: 0.1493 - val_accuracy: 0.9667 - val_loss: 0.1871\n",
            "Epoch 5/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 93ms/step - accuracy: 0.9708 - loss: 0.1435 - val_accuracy: 0.9666 - val_loss: 0.1879\n",
            "Epoch 6/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 93ms/step - accuracy: 0.9712 - loss: 0.1397 - val_accuracy: 0.9665 - val_loss: 0.1896\n",
            "Epoch 7/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 94ms/step - accuracy: 0.9716 - loss: 0.1370 - val_accuracy: 0.9667 - val_loss: 0.1903\n",
            "Epoch 8/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 94ms/step - accuracy: 0.9717 - loss: 0.1354 - val_accuracy: 0.9666 - val_loss: 0.1908\n",
            "Epoch 9/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 94ms/step - accuracy: 0.9720 - loss: 0.1340 - val_accuracy: 0.9665 - val_loss: 0.1921\n",
            "Epoch 10/10\n",
            "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 94ms/step - accuracy: 0.9723 - loss: 0.1315 - val_accuracy: 0.9666 - val_loss: 0.1926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open('cpp_tokenizer.pkl', 'rb') as f:\n",
        "    cpp_tokenizer = pickle.load(f)\n",
        "with open('pseudocode_tokenizer.pkl', 'rb') as f:\n",
        "    pseudocode_tokenizer = pickle.load(f)\n",
        "\n",
        "transformer = tf.keras.models.load_model('transformer_model.keras', compile=False)\n",
        "\n",
        "def generate_pseudocode(cpp_code, max_len=150):\n",
        "    input_seq = cpp_tokenizer.texts_to_sequences([\"<sos> \" + cpp_code + \" <eos>\"])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    pred_seq = transformer.predict(input_seq)\n",
        "    pred_indices = np.argmax(pred_seq, axis=-1)[0]\n",
        "    pseudo_tokens = [pseudocode_tokenizer.index_word.get(idx, '') for idx in pred_indices if idx > 0]\n",
        "\n",
        "    return ' '.join(pseudo_tokens).replace('<sos>', '').replace('<eos>', '').strip()\n",
        "\n",
        "cpp_example = \"int x\"\n",
        "pseudo_output = generate_pseudocode(cpp_example)\n",
        "print(\"Generated Pseudocode:\\n\", pseudo_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwOgiYSI1VQw",
        "outputId": "8ec433b9-0685-444a-ae3c-d83564c25025"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79ef2d2dad40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Generated Pseudocode:\n",
            " x integer x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgFk2PanDwmK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}